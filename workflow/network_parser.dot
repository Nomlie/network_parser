digraph NetworkParser_Flow_Detailed {
  // ─────────────────────────────────────────────────────────────
  // NetworkParser — detailed, script-faithful pipeline flow
  // Updated: replaces UNION-of-matrix logic + bcftools-merge logic
  // with DataLoader's per-sample parse → cohort merge → baseline → encode
  // (i.e., the DataLoader_Flow_AlignedToCode design).
  // ─────────────────────────────────────────────────────────────

  rankdir=TB;
  labelloc="t";
  fontsize=20;
  fontname="Helvetica,Arial,sans-serif";
  label="NetworkParser — Detailed Pipeline Flow \nInput → Matrix construction → Pre-tree statistical filtering → Decision tree → Post-tree confidence + epistasis → Optional validation → Final JSON";

  graph [splines=ortho, nodesep=0.45, ranksep=0.7];
  node  [shape=box, style="rounded,filled", fontname="Helvetica,Arial,sans-serif", fontsize=10, penwidth=1.15, fillcolor="#eef5ff"];
  edge  [fontname="Helvetica,Arial,sans-serif", fontsize=9, penwidth=1.1, arrowsize=0.9, color="#444"];

  // ─────────────────────────────────────────────────────────────
  // STAGE 0 — CLI
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_cli {
    label="Stage 0 — CLI entry (cli.py)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#c9d7f0";

    cli_args [label=<
      <B>Parse CLI args</B><BR ALIGN="LEFT"/>
      • --genomic: file OR directory (VCF/VCF.gz/CSV/TSV)<BR ALIGN="LEFT"/>
      • --meta: optional metadata table (CSV/TSV)<BR ALIGN="LEFT"/>
      • --label: metadata column for phenotype / group<BR ALIGN="LEFT"/>
      • --known_markers: optional marker list<BR ALIGN="LEFT"/>
      • --ref_fasta: optional FASTA for normalization (optional / context)<BR ALIGN="LEFT"/>
      • --config: JSON overrides<BR ALIGN="LEFT"/>
      • --validate_statistics / --validate_interactions flags
    >];

    cli_cfg [label=<
      <B>Load config</B><BR ALIGN="LEFT"/>
      NetworkParserConfig() defaults<BR ALIGN="LEFT"/>
      Apply JSON overrides
    >];

    cli_call [label=<
      <B>Call pipeline</B><BR ALIGN="LEFT"/>
      run_networkparser_analysis(...) → NetworkParser.run_pipeline(...)
    >];

    cli_args -> cli_cfg -> cli_call;
  }

  // ─────────────────────────────────────────────────────────────
  // ORCHESTRATOR
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_orch {
    label="Orchestrator (network_parser.py)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#c9d7f0";

    np_init [label=<
      <B>Instantiate components</B><BR ALIGN="LEFT"/>
      • DataLoader(config)<BR ALIGN="LEFT"/>
      • EnhancedDecisionTreeBuilder(config)<BR ALIGN="LEFT"/>
      • StatisticalValidator(config)
    >];

    np_run [label=<
      <B>run_pipeline()</B><BR ALIGN="LEFT"/>
      1) Build genomic matrix (DataLoader)<BR ALIGN="LEFT"/>
      2) Load metadata + labels<BR ALIGN="LEFT"/>
      3) Discover features + interactions (tree-based)<BR ALIGN="LEFT"/>
      4) Optional validation (if flags)<BR ALIGN="LEFT"/>
      5) Save final JSON report
    >];

    cli_call -> np_init -> np_run;
  }

  // ─────────────────────────────────────────────────────────────
  // STAGE 1 — DATA LOADER (UPDATED)
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_loader {
    label="Stage 1 — Matrix construction (data_loader.py) — per-sample parse → cohort merge → baseline → encode";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#d7e3f7";

    dl_route [label=<
      <B>load_genomic_matrix(genomic_path)</B><BR ALIGN="LEFT"/>
      Route by input type:<BR ALIGN="LEFT"/>
      • directory → folder-of-VCFs mode<BR ALIGN="LEFT"/>
      • .vcf / .vcf.gz → single-VCF mode (treated as single-sample or already-cohort VCF)<BR ALIGN="LEFT"/>
      • .csv / .tsv → prebuilt matrix mode<BR ALIGN="LEFT"/>
    >];

    // Directory mode: no VCF merge, no UNION-of-sites
    dl_dir [label=<
      <B>Directory input</B><BR ALIGN="LEFT"/>
      Collect *.vcf / *.vcf.gz<BR ALIGN="LEFT"/>
      (no bcftools merge; no union-of-sites pass)
    >];

    dl_par [label=<
      <B>Parallel per-sample parsing</B><BR ALIGN="LEFT"/>
      Parallel(n_jobs): parse each VCF independently<BR ALIGN="LEFT"/>
      Output per sample: {(chrom,pos) → (ref, called)}
    > shape=ellipse fillcolor="#FFD48A"];

    iter0 [label=<
      <B>iter_sample_calls()</B><BR ALIGN="LEFT"/>
      stream records from a single sample VCF<BR ALIGN="LEFT"/>
      apply record-level gates + choose allele
    > fillcolor="#fff7db"];

    gate_snp [label=<
      <B>is_snp_like?</B><BR ALIGN="LEFT"/>
      single-base REF &amp; ALT<BR ALIGN="LEFT"/>
      (optionally enforce biallelic-only)
    > shape=diamond fillcolor="#fff7db"];

    gate_info [label=<
      <B>passes_info_qc?</B><BR ALIGN="LEFT"/>
      QUAL + INFO(DP, MQ, MQ0F) thresholds<BR ALIGN="LEFT"/>
      (config-driven)
    > shape=diamond fillcolor="#fff7db"];

    choose0 [label=<
      <B>choose_called_allele()</B><BR ALIGN="LEFT"/>
      Prefer GT when available (FORMAT)<BR ALIGN="LEFT"/>
      Else treat ALT presence as called<BR ALIGN="LEFT"/>
      Fallback to REF when needed
    >];

    store0 [label=<
      <B>Store sample calls</B><BR ALIGN="LEFT"/>
      (chrom,pos) → (ref, called)
    >];

    discard [label="discard record" fillcolor="#E9E9E9" fontcolor="gray35"];

    // Cohort merge (within DataLoader)
    merge_calls [label=<
      <B>Cohort merge + carrier counts</B><BR ALIGN="LEFT"/>
      Merge all per-sample call maps<BR ALIGN="LEFT"/>
      Count carriers only where called ≠ ref
    > fillcolor="#E7F2FF"];

    bial_cons [label=<
      <B>Optional ALT consistency gate</B><BR ALIGN="LEFT"/>
      If biallelic-only, enforce consistent ALT per site<BR ALIGN="LEFT"/>
      (discard conflicting ALT)
    > shape=diamond fillcolor="#F2FAFF"];

    pres [label=<
      <B>Cohort presence filter</B><BR ALIGN="LEFT"/>
      keep sites where carrier_count ≥ min_sample_presence
    > shape=diamond fillcolor="#F2FAFF"];

    sort_sites [label=<
      <B>Sort retained sites</B><BR ALIGN="LEFT"/>
      sort by chrom, pos
    > fillcolor="#E7F2FF"];

    // Baseline + encoding
    refline [label=<
      <B>Build ref_line</B><BR ALIGN="LEFT"/>
      join REF bases across ordered sites
    > fillcolor="#D7F7D7"];

    alleles [label=<
      <B>Build per-sample allele strings</B><BR ALIGN="LEFT"/>
      called if valid else fallback REF<BR ALIGN="LEFT"/>
      also collect per-position base counts (Counter)
    > fillcolor="#D7F7D7"];

    baseQ [label=<
      <B>Choose baseline</B><BR ALIGN="LEFT"/>
      ancestral_allele == 'Y' ? REF : MODE
    > shape=diamond fillcolor="#ECFFEC"];

    base_ref [label="baseline = ref_line" fillcolor="#D7F7D7"];
    base_mode [label="baseline = per-position mode base" fillcolor="#D7F7D7"];

    encode [label=<
      <B>Binary encode</B><BR ALIGN="LEFT"/>
      0 = matches baseline<BR ALIGN="LEFT"/>
      1 = differs
    > fillcolor="#CFF7F7"];

    varids [label=<
      <B>Variant IDs</B><BR ALIGN="LEFT"/>
      CHROM:POS:REF:ALT
    > fillcolor="#CFF7F7"];

    build_df [label=<
      <B>Build DataFrame</B><BR ALIGN="LEFT"/>
      rows = samples<BR ALIGN="LEFT"/>
      cols = variant IDs<BR ALIGN="LEFT"/>
      values ∈ {0,1}
    > fillcolor="#CFF7F7"];

    // Light preprocessing (DataLoader only)
    pre0 [label=<
      <B>_preprocess_binary_matrix()</B><BR ALIGN="LEFT"/>
      optional: drop invariant cols (all 0 / all 1)<BR ALIGN="LEFT"/>
      optional: min_minor_count filter
    > fillcolor="#FFE0F0"];

    compactQ [label="use_integer_variant_ids?" shape=diamond fillcolor="#EDEDED"];
    compactY [label="rename → v0,v1,... + save lookup" fillcolor="#EDEDED"];
    compactN [label="keep full variant IDs" fillcolor="#EDEDED"];

    outQ [label="output_dir provided?" shape=diamond fillcolor="#FFE7E0"];
    write_all [label=<
      <B>_write_all_artifacts()</B><BR ALIGN="LEFT"/>
      • SNP table (minimal or annotated if ref/gbk available)<BR ALIGN="LEFT"/>
      • alleles.fasta + binary.fasta<BR ALIGN="LEFT"/>
      • filtered.tsv (+ optional context)<BR ALIGN="LEFT"/>
      • matrices/* (filtered derivatives)<BR ALIGN="LEFT"/>
      • config snapshot
    > fillcolor="#FFD6C9"];

    dl_return [label="Return binary DataFrame\n(samples × variants, 0/1)" shape=ellipse fillcolor="#D7F7D7"];

    // Wiring (Stage 1)
    np_run -> dl_route;

    dl_route -> dl_dir [label="if directory"];
    dl_dir -> dl_par;
    dl_par -> iter0 [label="for each sample VCF"];
    iter0 -> gate_snp;
    gate_snp -> discard [label="no" style=dashed color="gray50"];
    gate_snp -> gate_info [label="yes"];
    gate_info -> discard [label="no" style=dashed color="gray50"];
    gate_info -> choose0 [label="yes"];
    choose0 -> store0;

    store0 -> merge_calls [label="all samples done"];
    merge_calls -> bial_cons;
    bial_cons -> pres [label="ok"];

    pres -> sort_sites [label="kept"];
    pres -> discard [label="none left" style=dashed color="gray50"];

    sort_sites -> refline -> alleles -> baseQ;
    baseQ -> base_ref [label="Y"];
    baseQ -> base_mode [label="mode"];
    base_ref -> encode;
    base_mode -> encode;
    encode -> varids -> build_df -> pre0;

    pre0 -> compactQ;
    compactQ -> compactY [label="yes"];
    compactQ -> compactN [label="no"];

    compactY -> outQ;
    compactN -> outQ;

    outQ -> write_all [label="yes"];
    outQ -> dl_return [label="no"];
    write_all -> dl_return;

    // Single-VCF mode: reuses the same parse→encode logic (no merge-VCFs step)
    dl_single [label=<
      <B>Single VCF input</B><BR ALIGN="LEFT"/>
      Treat as a single-sample VCF (or already-cohort VCF parsed similarly)<BR ALIGN="LEFT"/>
      Reuse iter_sample_calls() → encode pipeline
    > fillcolor="#FFF1C9"];

    dl_route -> dl_single [label="if .vcf/.vcf.gz"];
    dl_single -> iter0;

    // Prebuilt matrix mode
    mat0 [label=<
      <B>CSV/TSV matrix mode</B><BR ALIGN="LEFT"/>
      Load sample × feature matrix as-is<BR ALIGN="LEFT"/>
      (assumes already numeric/binary)
    >];

    dl_route -> mat0 [label="if .csv/.tsv"];

    // Metadata + known markers
    meta0 [label=<
      <B>load_metadata(meta_path)</B><BR ALIGN="LEFT"/>
      Read CSV/TSV<BR ALIGN="LEFT"/>
      Index = "Sample" column if present else first column<BR ALIGN="LEFT"/>
      Save metadata.normalized.csv
    > fillcolor="#e9fff2"];

    km0 [label=<
      <B>load_known_markers(known_markers_path)</B><BR ALIGN="LEFT"/>
      Accept .txt (one per line) OR .csv/.tsv<BR ALIGN="LEFT"/>
      Column "marker" or first column<BR ALIGN="LEFT"/>
      Save known_markers.normalized.txt
    > fillcolor="#e9fff2"];

    dl_route -> meta0 [style=dashed, label="if --meta"];
    dl_route -> km0   [style=dashed, label="if --known_markers"];
  }

  // ─────────────────────────────────────────────────────────────
  // STAGE 2 — DISCOVERY (pre-tree filtering + decision tree)
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_discovery {
    label="Stage 2 — Feature discovery (decision_tree_builder.py)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#d7e3f7";

    disc_in [label=<
      <B>Inputs to discovery</B><BR ALIGN="LEFT"/>
      • genomic_matrix (samples × variants)<BR ALIGN="LEFT"/>
      • labels = metadata[label_column]<BR ALIGN="LEFT"/>
      • all_features = matrix columns
    >];

    pre1 [label=<
      <B>Pre-tree filtering (statistically defensible)</B><BR ALIGN="LEFT"/>
      1) Missingness gate: keep features with sufficient non-missing fraction<BR ALIGN="LEFT"/>
      2) MAF gate: keep features with MAF ≥ threshold<BR ALIGN="LEFT"/>
      3) Per-feature association test vs label:<BR ALIGN="LEFT"/>
         • Fisher for 2×2, else χ²<BR ALIGN="LEFT"/>
      4) Multiple testing correction: FDR-BH<BR ALIGN="LEFT"/>
      5) If none pass → fallback top-by-variance subset
    >];

    misslog [label=<
      <B>Missingness reporting</B><BR ALIGN="LEFT"/>
      Compute per-feature NaN fraction<BR ALIGN="LEFT"/>
      Log counts only (avoid printing thousands)
    > fillcolor="#fff2cc"];

    impute0 [label=<
      <B>Impute missing for tree fit</B><BR ALIGN="LEFT"/>
      NaN → 0 (treat missing as baseline)<BR ALIGN="LEFT"/>
      Drop near-monomorphic after imputation
    >];

    tree0 [label=<
      <B>Train decision tree</B><BR ALIGN="LEFT"/>
      DecisionTreeClassifier(max_depth, min_samples_split, min_samples_leaf, random_state)<BR ALIGN="LEFT"/>
      Labels encoded with LabelEncoder
    >];

    rule0 [label=<
      <B>Interpretability outputs</B><BR ALIGN="LEFT"/>
      export_text(...) rules<BR ALIGN="LEFT"/>
      Root features = depth 0<BR ALIGN="LEFT"/>
      Branch features = depth &gt; 0
    >];

    conf0 [label=<
      <B>Confidence scoring (post-tree)</B><BR ALIGN="LEFT"/>
      For features used in tree:<BR ALIGN="LEFT"/>
      • Mutual information(feature, label)<BR ALIGN="LEFT"/>
      • Bootstrap stability of feature usage/importance<BR ALIGN="LEFT"/>
      • Effect size proxy: Cramér’s V<BR ALIGN="LEFT"/>
      Weighted aggregate → confidence score<BR ALIGN="LEFT"/>
      Save feature_confidence.json
    >];

    epi0 [label=<
      <B>Epistatic interaction mining (post-tree)</B><BR ALIGN="LEFT"/>
      Traverse root→leaf paths<BR ALIGN="LEFT"/>
      For adjacent feature pairs on a path:<BR ALIGN="LEFT"/>
      • compute MI-based synergy proxy<BR ALIGN="LEFT"/>
      • keep if ≥ epistasis_strength_threshold<BR ALIGN="LEFT"/>
      Cap number of interactions<BR ALIGN="LEFT"/>
      Save epistatic_interactions.json
    >];

    files0 [label=<
      <B>Files written</B><BR ALIGN="LEFT"/>
      • decision_tree_rules.txt<BR ALIGN="LEFT"/>
      • feature_confidence.json<BR ALIGN="LEFT"/>
      • epistatic_interactions.json
    > fillcolor="#e9fff2"];

    // connect discovery
    dl_return -> disc_in [label="matrix"];
    mat0 -> disc_in [label="matrix"];
    meta0 -> disc_in [style=dashed, label="labels from meta[label_column]"];

    disc_in -> pre1 -> misslog -> impute0 -> tree0 -> rule0 -> conf0 -> epi0 -> files0;
  }

  // ─────────────────────────────────────────────────────────────
  // STAGE 3 — VALIDATION (optional)
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_validation {
    label="Stage 3 — Optional validation (statistical_validation.py)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#d7e3f7";

    v_in [label=<
      <B>Validation inputs</B><BR ALIGN="LEFT"/>
      • matrix (samples × variants)<BR ALIGN="LEFT"/>
      • labels<BR ALIGN="LEFT"/>
      • discovered features / interactions (from Stage 2)
    >];

    v_assoc [label=<
      <B>Feature association tests</B><BR ALIGN="LEFT"/>
      chi_squared_test(): χ²/Fisher + MI + Cramér’s V<BR ALIGN="LEFT"/>
      Save chi_squared_results.json
    >];

    v_mtest [label=<
      <B>Multiple testing correction</B><BR ALIGN="LEFT"/>
      multiple_testing_correction(): FDR-BH / Bonferroni<BR ALIGN="LEFT"/>
      Save multiple_testing_results.json
    >];

    v_boot [label=<
      <B>Bootstrap validation</B><BR ALIGN="LEFT"/>
      bootstrap_validation(): stability &amp; importance distributions<BR ALIGN="LEFT"/>
      Save bootstrap_results.json
    >];

    v_perm [label=<
      <B>Interaction permutation tests</B><BR ALIGN="LEFT"/>
      permutation_test_interactions(): permutation p-values for candidate pairs<BR ALIGN="LEFT"/>
      Save interaction_permutation_results.json
    >];

    v_note [label=<
      <B>Implementation note</B><BR ALIGN="LEFT"/>
      Orchestrator calls validate_features()/validate_interactions()<BR ALIGN="LEFT"/>
      but StatisticalValidator exposes chi_squared_test(), multiple_testing_correction(),<BR ALIGN="LEFT"/>
      bootstrap_validation(), permutation_test_interactions().<BR ALIGN="LEFT"/>
      Wrapper methods or call-site update required for flags to work.
    > fillcolor="#ffe6e6"];

    // connect validation
    files0 -> v_in [style=dashed, label="if flags enabled"];
    v_in -> v_assoc -> v_mtest -> v_boot;
    v_in -> v_perm;
    v_note -> v_in [style=dotted];
  }

  // ─────────────────────────────────────────────────────────────
  // STAGE 4 — FINAL REPORT
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_report {
    label="Stage 4 — Final synthesis (network_parser.py + utils.py)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#c9d7f0";

    out0 [label=<
      <B>Assemble results dictionary</B><BR ALIGN="LEFT"/>
      • timestamp()<BR ALIGN="LEFT"/>
      • config snapshot vars(config)<BR ALIGN="LEFT"/>
      • discovery outputs (features, rules, confidence, interactions)<BR ALIGN="LEFT"/>
      • validation outputs (if executed)
    >];

    out1 [label=<
      <B>Write final JSON</B><BR ALIGN="LEFT"/>
      save_json(networkparser_results_&lt;timestamp&gt;.json)<BR ALIGN="LEFT"/>
      ensure_dir(output_dir)
    > fillcolor="#e9fff2"];
  }

  // wire final stage
  np_run -> out0;
  files0 -> out0;
  v_boot -> out0 [style=dashed];
  v_perm -> out0 [style=dashed];
  out0 -> out1;

  // ─────────────────────────────────────────────────────────────
  // Dependency callouts (what it uses)
  // ─────────────────────────────────────────────────────────────
  subgraph cluster_deps {
    label="What it uses (tools + libraries)";
    labelloc="t";
    fontsize=14;
    style="rounded";
    color="#e2e2e2";

    dep_py [label=<
      <B>Python</B><BR ALIGN="LEFT"/>
      • pandas, numpy<BR ALIGN="LEFT"/>
      • scikit-learn (DecisionTreeClassifier, LabelEncoder, MI)<BR ALIGN="LEFT"/>
      • scipy.stats, statsmodels (FDR-BH etc.)<BR ALIGN="LEFT"/>
      • joblib (parallel validation)<BR ALIGN="LEFT"/>
      • optional: Biopython (SNP annotation / context if enabled)
    > fillcolor="#f4f4f4"];
  }

  // connect deps lightly
  dl_par -> dep_py [style=dotted];
  pre1 -> dep_py [style=dotted];
  v_assoc -> dep_py [style=dotted];
  write_all -> dep_py [style=dotted];
}