# NetworkParser Pipeline Manual

## Overview
The `network_parser` pipeline is a Python-based tool designed for analyzing genomic data to identify significant features (e.g., SNPs) associated with a specified label (e.g., phenotype groups) and to construct interaction networks. It processes genomic matrices, performs statistical validation, builds decision trees, and generates network graphs. The pipeline is executed via a command-line interface (CLI) and consists of four main stages: Input Processing, Feature Discovery, Statistical Validation, and Integration. This manual describes the key scripts involved, their functionalities, inputs, outputs, and roles in the pipeline.

The pipeline is invoked using:
```bash
python -m network_parser.cli --genomic <path_to_csv> --label <label_column> --output-dir <output_directory>
```

### Pipeline Stages
1. **Input Processing**: Loads and preprocesses genomic data, aligning features and labels, removing invariant features.
2. **Feature Discovery**: Identifies significant features using statistical tests and builds a decision tree.
3. **Statistical Validation**: Validates discovered features using bootstrap and permutation tests.
4. **Integration**: Ranks features and generates network graphs (e.g., in GraphML format).

## Scripts and Their Functions

### 1. `cli.py`
**Purpose**: Serves as the command-line interface to initiate the `network_parser` pipeline.

**Functionality**:
- Parses command-line arguments (e.g., `--genomic`, `--label`, `--output-dir`).
- Initializes the `NetworkParserConfig` with parameters like `max_depth`, `min_group_size`, `significance_level`, `n_bootstrap_samples`, `n_permutation_tests`, `multiple_testing_method`, `min_information_gain`, `n_jobs`, and `random_state`.
- Calls `network_parser.py` to execute the pipeline stages.
- Logs pipeline progress and errors.

**Inputs**:
- `--genomic`: Path to the input genomic CSV file (e.g., `example.csv` with sample IDs, features, and a label column).
- `--label`: Name of the label column in the CSV (e.g., `Group`).
- `--output-dir`: Directory to save output files (e.g., `results/`).

**Outputs**:
- Coordinates output files generated by other modules, saved in `output-dir`.
- Log file (`pipeline_run.log`) capturing pipeline execution details.

**Role in Pipeline**:
- Acts as the entry point, passing arguments to `network_parser.py` for execution.
- Example log: `2025-09-23 15:13:12,938 - INFO - Starting NetworkParser pipeline`.

**Dependencies**:
- `argparse`, `logging`, `pathlib`, `network_parser.config`, `network_parser.network_parser`.

---

### 2. `config.py`
**Purpose**: Defines the configuration class for the pipeline.

**Functionality**:
- Provides the `NetworkParserConfig` class to store pipeline parameters.
- Sets default values for parameters like `max_depth=None`, `min_group_size=5`, `significance_level=0.05`, `n_bootstrap_samples=1000`, `n_permutation_tests=500`, `multiple_testing_method='fdr_bh'`, `min_information_gain=0.001`, `n_jobs=-1`, and `random_state=42`.
- Allows customization via command-line arguments or configuration files.

**Inputs**:
- Command-line arguments parsed by `cli.py` or a configuration file (if supported).

**Outputs**:
- A `NetworkParserConfig` object passed to other modules.

**Role in Pipeline**:
- Centralizes configuration, ensuring consistent parameter usage across all stages.
- Example log: `2025-09-23 15:13:12,938 - INFO - Initializing NetworkParser with config: {...}`.

**Dependencies**:
- Standard Python libraries (e.g., `dataclasses` or `typing`).

---

### 3. `data_loader.py`
**Purpose**: Handles data loading, preprocessing, and alignment (Stage 1: Input Processing).

**Functionality**:
- Loads the genomic CSV file (e.g., `example.csv`) containing sample IDs, feature columns (e.g., SNPs), and a label column (e.g., `Group`).
- Deduplicates data to remove redundant entries.
- Aligns genomic data and metadata, ensuring consistent indices between features and labels.
- Removes invariant features (e.g., SNPs with no variation across samples).
- Saves preprocessed data to files like `deduplicated_genomic_matrix.csv`, `aligned_genomic_matrix.csv`, and `aligned_metadata.csv`.

**Inputs**:
- Path to the genomic CSV file.
- Label column name.
- Output directory for saving preprocessed files.

**Outputs**:
- `deduplicated_genomic_matrix.csv`: Deduplicated genomic data.
- `aligned_genomic_matrix.csv`: Aligned feature matrix.
- `aligned_metadata.csv`: Aligned label data.
- Log messages, e.g., `2025-09-23 15:13:12,972 - INFO - Aligned data: 15 samples, 22 features retained`.

**Role in Pipeline**:
- Prepares clean, aligned data for subsequent analysis, reducing the initial 89 features to 22 by removing 67 invariants.
- Example log: `2025-09-23 15:13:12,971 - INFO - Removing 67 invariant features`.

**Dependencies**:
- `pandas`, `numpy`, `logging`, `pathlib`, `network_parser.config`.

---

### 4. `decision_tree_builder.py`
**Purpose**: Performs feature discovery and builds a decision tree (Stage 2: Feature Discovery).

**Functionality**:
- Conducts statistical association tests (e.g., chi-squared or Fisher’s exact tests) on features using `StatisticalValidator.chi_squared_test`.
- Applies multiple testing correction (e.g., FDR-BH) to filter significant features (e.g., 7 out of 22 features).
- Builds a decision tree using the filtered features, with constraints like `max_depth`, `min_group_size`, and `min_information_gain`.
- Identifies root features (e.g., `3219764`) and potential epistatic interactions (none in this case).
- Saves results like `decision_tree_rules.txt`, `feature_confidence.json`, and `epistatic_interactions.json`.

**Inputs**:
- Aligned genomic data (DataFrame) and labels (Series) from `data_loader.py`.
- Configuration parameters (e.g., `max_depth`, `min_information_gain`).
- Output directory.

**Outputs**:
- `decision_tree_rules.txt`: Text representation of the decision tree.
- `feature_confidence.json`: Confidence scores for discovered features.
- `epistatic_interactions.json`: Epistatic interaction details (empty if none found).
- Log messages, e.g., `2025-09-23 15:13:19,705 - INFO - Built decision tree with depth 1 and 2 leaves`.

**Role in Pipeline**:
- Identifies significant features and constructs a decision tree to model feature-label relationships.
- Example log: `2025-09-23 15:13:19,649 - INFO - Filtered 7 significant features`.

**Dependencies**:
- `sklearn.tree`, `pandas`, `numpy`, `logging`, `pathlib`, `network_parser.config`, `network_parser.statistical_validator`.

---

### 5. `statistical_validator.py`
**Purpose**: Validates discovered features using statistical methods (Stage 3: Statistical Validation).

**Functionality**:
- **Chi-Squared/Fisher’s Exact Tests**: Tests feature-label associations, computing p-values, Cramér’s V, and mutual information. Saves results to `chi_squared_results.json`.
- **Multiple Testing Correction**: Applies FDR-BH correction to p-values, saving to `multiple_testing_results.json`.
- **Bootstrap Validation**: Assesses feature stability using 1000 bootstrap samples, computing stability scores, mean importance, confidence intervals, and p-values. Saves to `bootstrap_results.json`.
- **Permutation Tests for Interactions**: Evaluates epistatic interactions (none in this case) using 500 permutations, saving to `interaction_permutation_results.json`.
- Uses parallel processing (`joblib`) for efficiency.

**Inputs**:
- Aligned genomic data and labels from `data_loader.py`.
- Discovered features and interactions from `decision_tree_builder.py`.
- Configuration parameters (e.g., `n_bootstrap_samples`, `n_permutation_tests`, `significance_level`).
- Output directory.

**Outputs**:
- `chi_squared_results.json`: Feature association test results.
- `multiple_testing_results.json`: Corrected p-values and significance flags.
- `bootstrap_results.json`: Feature stability scores, importance, and confidence intervals.
- `interaction_permutation_results.json`: Interaction strengths and p-values (empty if no interactions).
- Log messages, e.g., `2025-09-23 15:13:20,373 - INFO - Saved bootstrap results to: results/`.

**Role in Pipeline**:
- Validates the significance and stability of discovered features and interactions.
- Example log: `2025-09-23 15:13:19,724 - INFO - Running bootstrap validation with 1000 samples`.

**Dependencies**:
- `scipy.stats`, `statsmodels.stats.multitest`, `sklearn.tree`, `sklearn.metrics`, `pandas`, `numpy`, `joblib`, `logging`, `pathlib`, `json`, `network_parser.config`.

---

### 6. `network_parser.py`
**Purpose**: Orchestrates the pipeline and handles integration (Stage 4: Integration).

**Functionality**:
- Coordinates the execution of `data_loader.py`, `decision_tree_builder.py`, and `statistical_validator.py` for Stages 1–3.
- Combines feature discovery and validation results for Stage 4.
- Ranks features by confidence scores.
- Constructs a network graph using `networkx`, with nodes as features (e.g., `3219764`) and edges for epistatic interactions (none in this case).
- Computes network properties like degree centrality and clustering coefficients.
- Saves the network graph to `network_graph.graphml` and integrated results to `networkparser_results_*.json`.

**Inputs**:
- Configuration from `cli.py`.
- Aligned data and labels from `data_loader.py`.
- Discovery results from `decision_tree_builder.py` (e.g., features, confidences, interactions).
- Validation results from `statistical_validator.py` (e.g., stability scores).
- Output directory.

**Outputs**:
- `network_graph.graphml`: Network graph in GraphML format.
- `networkparser_results_*.json`: Integrated results with ranked features and network properties.
- Log messages, e.g., `2025-09-23 15:13:21,043 - INFO - Saved network graphs to GraphML files`.

**Role in Pipeline**:
- Manages the entire pipeline execution and finalizes the analysis by ranking features and visualizing interactions as a network.
- Example log: `2025-09-23 15:13:20,373 - INFO - Ranked 1 features by confidence`.

**Dependencies**:
- `networkx`, `pandas`, `numpy`, `logging`, `pathlib`, `json`, `network_parser.config`, `network_parser.data_loader`, `network_parser.decision_tree_builder`, `network_parser.statistical_validator`.

---

## Pipeline Workflow
1. **cli.py**:
   - Parses arguments and initializes `NetworkParserConfig`.
   - Calls `network_parser.py` to execute the pipeline.
2. **network_parser.py**:
   - Coordinates Stages 1–4.
   - Calls `data_loader.py` for Stage 1, `decision_tree_builder.py` for Stage 2, `statistical_validator.py` for Stage 3, and handles Stage 4 (Integration).
3. **data_loader.py**:
   - Loads and preprocesses `example.csv`, removing 67 invariant features, resulting in 15 samples and 22 features.
   - Saves `deduplicated_genomic_matrix.csv`, `aligned_genomic_matrix.csv`, and `aligned_metadata.csv`.
4. **decision_tree_builder.py**:
   - Uses `statistical_validator.py` to run chi-squared/Fisher’s exact tests and FDR-BH correction, filtering to 7 significant features.
   - Builds a decision tree (depth 1, 2 leaves, accuracy 1.0) with root feature `3219764`.
   - Saves `decision_tree_rules.txt`, `feature_confidence.json`, `epistatic_interactions.json`.
5. **statistical_validator.py**:
   - Performs bootstrap validation (1000 samples) and permutation tests (500 iterations).
   - Saves `chi_squared_results.json`, `multiple_testing_results.json`, `bootstrap_results.json`, `interaction_permutation_results.json`.
6. **network_parser.py (Integration)**:
   - Ranks features and builds a network graph.
   - Saves `network_graph.graphml` and `networkparser_results_*.json`.

## Output Files
- **results/deduplicated_genomic_matrix.csv**: Deduplicated input data.
- **results/aligned_genomic_matrix.csv**: Aligned feature matrix.
- **results/aligned_metadata.csv**: Aligned labels.
- **results/chi_squared_results.json**: Feature association test results.
- **results/multiple_testing_results.json**: Corrected p-values.
- **results/decision_tree_rules.txt**: Decision tree structure.
- **results/feature_confidence.json**: Feature confidence scores.
- **results/epistatic_interactions.json**: Interaction details (empty in this case).
- **results/bootstrap_results.json**: Feature stability and importance.
- **results/interaction_permutation_results.json**: Interaction validation (empty in this case).
- **results/network_graph.graphml**: Network graph.
- **results/networkparser_results_*.json**: Integrated results.
- **pipeline_run.log**: Execution log.

## Notes
- The dataset (`example.csv`) is small (15 samples), resulting in a simple decision tree (depth 1) and no epistatic interactions. Larger datasets may yield more complex models.
- The pipeline uses parallel processing (`joblib`) for efficiency in statistical tests and validation.
- Configuration parameters (e.g., `n_bootstrap_samples=1000`, `n_permutation_tests=500`) can be adjusted via `cli.py` arguments.

## Troubleshooting
- Ensure dependencies are installed: `conda install scipy statsmodels scikit-learn pandas numpy joblib networkx`.
- Check `example.csv` format: `sample_id,feature1,feature2,...,Group`.
- If errors occur, review `pipeline_run.log` and verify script imports (e.g., `import numpy as np` in `network_parser.py`).