# NetworkParser Pipeline Manual

## Overview

**NetworkParser** is a scalable, modular, and interpretable bioinformatics pipeline designed for microbial genomic analysis. It identifies statistically validated genomic markers and epistatic interactions driving phenotypic segregation (e.g., antimicrobial resistance, lineage diversification) while simultaneously producing phylogenetic-ready outputs.

The pipeline processes variant call format (VCF) files and phenotypic metadata to generate clean binary matrices, decision tree-based feature rankings, statistically robust validation, interaction networks, and consensus pseudogenomes suitable for phylogenetic reconstruction.

### Key Features 

- Native support for VCF(.gz) input with high-quality biallelic SNP/indel filtering (using bcftools)
- Generation of consensus pseudogenome FASTA files (`bcftools consensus`)
- Clean binary SNP matrix optimized for machine learning and epistasis analysis
- Interpretable decision tree-based feature discovery distinguishing root vs. branch markers
- Rigorous statistical validation including bootstrap resampling and permutation testing
- Rich network outputs: sample-feature bipartite graphs + epistatic interaction graphs (GraphML format + GNN-ready matrices)
- End-to-end reproducibility through conda-based environment

### Pipeline Stages

1. **Input Processing**  
   Loading, quality filtering, variant normalization, binary matrix creation, and consensus sequence generation

2. **Feature Discovery**  
   Statistical association testing + decision tree construction for marker discovery and epistasis detection

3. **Statistical Validation**  
   Bootstrap-based stability assessment + permutation testing for interaction significance

4. **Integration & Network Construction**  
   Feature ranking, bipartite sample-feature networks, and epistatic interaction graphs

5. **Output Generation**  
   Structured reports, GraphML networks, GNN-compatible matrices, and phylogenetic-ready FASTA files
## Quick Start Example — Mycobacterium tuberculosis Lineage Analysis

```bash
# Recommended: activate dedicated conda environment first
conda activate networkparser-env

python -m network_parser.cli \
  --genomic  data/tb_isolates.vcf.gz \
  --ref-fasta reference/H37Rv.fasta \
  --label    Lineage \
  --output-dir results_tb_2026/ \
  --n-jobs   -1 \
  --n-bootstrap 1000 \
  --n-permutations 500
Main outputs produced in results_tb_2026/:
File pattern / DirectoryDescriptiongenomic_matrix.csvClean binary SNP matrix (ML / epistasis ready)filtered_snps.final.vcf.gzHigh-quality filtered variant callsconsensus_fastas/*.fasta or all_samples_consensus.fastaIndividual or concatenated consensus pseudogenomessample_feature_network.graphmlBipartite sample–feature network (visualise in Cytoscape)interaction_graph.graphmlGraph of significant epistatic interactionsignn_matrices.npzGNN-ready adjacency/feature/label matricesnetworkparser_results_*.jsonComplete feature discovery + validation + ranking reportpipeline.logDetailed timestamped execution log
Follow-up phylogenetic analysis example:
Bashiqtree2 -s results_tb_2026/consensus_fastas/all_samples_consensus.fasta \
        -m GTR \
        -bb 1000 \
        -nt AUTO \
        --prefix tb_lineage_iqtree   
## Scripts and Their Functions

### 1. `cli.py`
**Purpose**: Serves as the command-line interface to initiate the `network_parser` pipeline.

**Functionality**:
- Parses command-line arguments (e.g., `--genomic`, `--label`, `--output-dir`).
- Initializes the `NetworkParserConfig` with parameters like `max_depth`, `min_group_size`, `significance_level`, `n_bootstrap_samples`, `n_permutation_tests`, `multiple_testing_method`, `min_information_gain`, `n_jobs`, and `random_state`.
- Calls `network_parser.py` to execute the pipeline stages.
- Logs pipeline progress and errors.

**Inputs**:
- `--genomic`: Path to the input genomic CSV file (e.g., `example.csv` with sample IDs, features, and a label column).
- `--label`: Name of the label column in the CSV (e.g., `Group`).
- `--output-dir`: Directory to save output files (e.g., `results/`).

**Outputs**:
- Coordinates output files generated by other modules, saved in `output-dir`.
- Log file (`pipeline_run.log`) capturing pipeline execution details.

**Role in Pipeline**:
- Acts as the entry point, passing arguments to `network_parser.py` for execution.
- Example log: `2025-09-23 15:13:12,938 - INFO - Starting NetworkParser pipeline`.

**Dependencies**:
- `argparse`, `logging`, `pathlib`, `network_parser.config`, `network_parser.network_parser`.

---
### 2. `config.py`
**Purpose**: Defines the configuration class for the pipeline.

**Functionality**:
- Provides the `NetworkParserConfig` class to store pipeline parameters.
- Sets default values for parameters like `max_depth=None`, `min_group_size=5`, `significance_level=0.05`, `n_bootstrap_samples=1000`, `n_permutation_tests=500`, `multiple_testing_method='fdr_bh'`, `min_information_gain=0.001`, `n_jobs=-1`, and `random_state=42`.
- Allows customization via command-line arguments or configuration files.

**Inputs**:
- Command-line arguments parsed by `cli.py` or a configuration file (if supported).

**Outputs**:
- A `NetworkParserConfig` object passed to other modules.

**Role in Pipeline**:
- Centralizes configuration, ensuring consistent parameter usage across all stages.
- Example log: `2025-09-23 15:13:12,938 - INFO - Initializing NetworkParser with config: {...}`.

**Dependencies**:
- Standard Python libraries (e.g., `dataclasses` or `typing`).

---

### 3. `data_loader.py`

**Purpose**: Handles all input processing tasks including modern microbial genomics formats (Stage 1: Input Processing).

**Functionality**:
- Supports native loading of compressed VCF(.gz) files
- Performs high-quality variant filtering (biallelic SNPs/indels, quality thresholds, missingness filtering)
- Generates clean binary SNP matrix (0/1/NA or similar encoding)
- Creates consensus pseudogenome FASTA files using `bcftools consensus` against a provided reference
- Optionally concatenates all samples into a single multiple-sequence FASTA suitable for phylogenetic reconstruction
- Deduplicates samples, aligns features and labels, removes invariant sites
- Saves key intermediate files for traceability and downstream analysis

**Inputs**:
- VCF(.gz) file containing variant calls
- Reference FASTA (required for consensus sequence generation)
- Label column name
- Output directory

**Outputs**:
- `genomic_matrix.csv` — clean binary variant matrix
- `filtered_snps.final.vcf.gz` — quality-filtered variant calls
- `consensus_fastas/*.fasta` or `all_samples_consensus.fasta` — pseudogenome(s) for phylogeny
- `deduplicated_genomic_matrix.csv`, `aligned_genomic_matrix.csv`, `aligned_metadata.csv`
- Log messages documenting filtering statistics and retained variants

**Role in Pipeline**:
Serves as the critical modern genomics-aware entry point, transforming raw variant calls into analysis-ready matrices and phylogenetic inputs while maintaining full reproducibility.

**Dependencies**:
- `pandas`, `numpy`, `bcftools` (via subprocess), `pathlib`, `logging`, `network_parser.config`

---

### 4. `decision_tree_builder.py`
**Purpose**: Performs feature discovery and builds a decision tree (Stage 2: Feature Discovery).

**Functionality**:
- Conducts statistical association tests (e.g., chi-squared or Fisher’s exact tests) on features using `StatisticalValidator.chi_squared_test`.
- Applies multiple testing correction (e.g., FDR-BH) to filter significant features (e.g., 7 out of 22 features).
- Builds a decision tree using the filtered features, with constraints like `max_depth`, `min_group_size`, and `min_information_gain`.
- Identifies root features (e.g., `3219764`) and potential epistatic interactions (none in this case).
- Saves results like `decision_tree_rules.txt`, `feature_confidence.json`, and `epistatic_interactions.json`.

**Inputs**:
- Aligned genomic data (DataFrame) and labels (Series) from `data_loader.py`.
- Configuration parameters (e.g., `max_depth`, `min_information_gain`).
- Output directory.

**Outputs**:
- `decision_tree_rules.txt`: Text representation of the decision tree.
- `feature_confidence.json`: Confidence scores for discovered features.
- `epistatic_interactions.json`: Epistatic interaction details (empty if none found).
- Log messages, e.g., `2025-09-23 15:13:19,705 - INFO - Built decision tree with depth 1 and 2 leaves`.

**Role in Pipeline**:
- Identifies significant features and constructs a decision tree to model feature-label relationships.
- Example log: `2025-09-23 15:13:19,649 - INFO - Filtered 7 significant features`.

**Dependencies**:
- `sklearn.tree`, `pandas`, `numpy`, `logging`, `pathlib`, `network_parser.config`, `network_parser.statistical_validator`.

---

### 5. `statistical_validator.py`
**Purpose**: Validates discovered features using statistical methods (Stage 3: Statistical Validation).

**Functionality**:
- **Chi-Squared/Fisher’s Exact Tests**: Tests feature-label associations, computing p-values, Cramér’s V, and mutual information. Saves results to `chi_squared_results.json`.
- **Multiple Testing Correction**: Applies FDR-BH correction to p-values, saving to `multiple_testing_results.json`.
- **Bootstrap Validation**: Assesses feature stability using 1000 bootstrap samples, computing stability scores, mean importance, confidence intervals, and p-values. Saves to `bootstrap_results.json`.
- **Permutation Tests for Interactions**: Evaluates epistatic interactions (none in this case) using 500 permutations, saving to `interaction_permutation_results.json`.
- Uses parallel processing (`joblib`) for efficiency.

**Inputs**:
- Aligned genomic data and labels from `data_loader.py`.
- Discovered features and interactions from `decision_tree_builder.py`.
- Configuration parameters (e.g., `n_bootstrap_samples`, `n_permutation_tests`, `significance_level`).
- Output directory.

**Outputs**:
- `chi_squared_results.json`: Feature association test results.
- `multiple_testing_results.json`: Corrected p-values and significance flags.
- `bootstrap_results.json`: Feature stability scores, importance, and confidence intervals.
- `interaction_permutation_results.json`: Interaction strengths and p-values (empty if no interactions).
- Log messages, e.g., `2025-09-23 15:13:20,373 - INFO - Saved bootstrap results to: results/`.

**Role in Pipeline**:
- Validates the significance and stability of discovered features and interactions.
- Example log: `2025-09-23 15:13:19,724 - INFO - Running bootstrap validation with 1000 samples`.

**Dependencies**:
- `scipy.stats`, `statsmodels.stats.multitest`, `sklearn.tree`, `sklearn.metrics`, `pandas`, `numpy`, `joblib`, `logging`, `pathlib`, `json`, `network_parser.config`.

---

### 6. `network_parser.py`
**Purpose**: Orchestrates the pipeline and handles integration (Stage 4: Integration).

**Functionality**:
- Coordinates the execution of `data_loader.py`, `decision_tree_builder.py`, and `statistical_validator.py` for Stages 1–3.
- Combines feature discovery and validation results for Stage 4.
- Ranks features by confidence scores.
- Constructs a network graph using `networkx`, with nodes as features (e.g., `3219764`) and edges for epistatic interactions (none in this case).
- Computes network properties like degree centrality and clustering coefficients.
- Saves the network graph to `network_graph.graphml` and integrated results to `networkparser_results_*.json`.

**Inputs**:
- Configuration from `cli.py`.
- Aligned data and labels from `data_loader.py`.
- Discovery results from `decision_tree_builder.py` (e.g., features, confidences, interactions).
- Validation results from `statistical_validator.py` (e.g., stability scores).
- Output directory.

**Outputs**:
- `network_graph.graphml`: Network graph in GraphML format.
- `networkparser_results_*.json`: Integrated results with ranked features and network properties.
- Log messages, e.g., `2025-09-23 15:13:21,043 - INFO - Saved network graphs to GraphML files`.

**Role in Pipeline**:
- Manages the entire pipeline execution and finalizes the analysis by ranking features and visualizing interactions as a network.
- Example log: `2025-09-23 15:13:20,373 - INFO - Ranked 1 features by confidence`.

**Dependencies**:
- `networkx`, `pandas`, `numpy`, `logging`, `pathlib`, `json`, `network_parser.config`, `network_parser.data_loader`, `network_parser.decision_tree_builder`, `network_parser.statistical_validator`.

---

## Pipeline Workflow
1. **cli.py**:
   - Parses arguments and initializes `NetworkParserConfig`.
   - Calls `network_parser.py` to execute the pipeline.
2. **network_parser.py**:
   - Coordinates Stages 1–4.
   - Calls `data_loader.py` for Stage 1, `decision_tree_builder.py` for Stage 2, `statistical_validator.py` for Stage 3, and handles Stage 4 (Integration).
3. **data_loader.py**:
   - Loads and preprocesses `example.csv`, removing 67 invariant features, resulting in 15 samples and 22 features.
   - Saves `deduplicated_genomic_matrix.csv`, `aligned_genomic_matrix.csv`, and `aligned_metadata.csv`.
4. **decision_tree_builder.py**:
   - Uses `statistical_validator.py` to run chi-squared/Fisher’s exact tests and FDR-BH correction, filtering to 7 significant features.
   - Builds a decision tree (depth 1, 2 leaves, accuracy 1.0) with root feature `3219764`.
   - Saves `decision_tree_rules.txt`, `feature_confidence.json`, `epistatic_interactions.json`.
5. **statistical_validator.py**:
   - Performs bootstrap validation (1000 samples) and permutation tests (500 iterations).
   - Saves `chi_squared_results.json`, `multiple_testing_results.json`, `bootstrap_results.json`, `interaction_permutation_results.json`.
6. **network_parser.py (Integration)**:
   - Ranks features and builds a network graph.
   - Saves `network_graph.graphml` and `networkparser_results_*.json`.

## Output Files
- **results/deduplicated_genomic_matrix.csv**: Deduplicated input data.
- **results/aligned_genomic_matrix.csv**: Aligned feature matrix.
- **results/aligned_metadata.csv**: Aligned labels.
- **results/chi_squared_results.json**: Feature association test results.
- **results/multiple_testing_results.json**: Corrected p-values.
- **results/decision_tree_rules.txt**: Decision tree structure.
- **results/feature_confidence.json**: Feature confidence scores.
- **results/epistatic_interactions.json**: Interaction details (empty in this case).
- **results/bootstrap_results.json**: Feature stability and importance.
- **results/interaction_permutation_results.json**: Interaction validation (empty in this case).
- **results/network_graph.graphml**: Network graph.
- **results/networkparser_results_*.json**: Integrated results.
- **pipeline_run.log**: Execution log.

## Notes
- The dataset (`example.csv`) is small (15 samples), resulting in a simple decision tree (depth 1) and no epistatic interactions. Larger datasets may yield more complex models.
- The pipeline uses parallel processing (`joblib`) for efficiency in statistical tests and validation.
- Configuration parameters (e.g., `n_bootstrap_samples=1000`, `n_permutation_tests=500`) can be adjusted via `cli.py` arguments.

## Troubleshooting
- Ensure dependencies are installed: `conda install scipy statsmodels scikit-learn pandas numpy joblib networkx`.
- Check `example.csv` format: `sample_id,feature1,feature2,...,Group`.
- If errors occur, review `pipeline_run.log` and verify script imports (e.g., `import numpy as np` in `network_parser.py`).